# @package _global_
defaults:
  - /pretrain_model: prior
  - /image_transformation
  - /callbacks:  
      - model_checkpoint
      - upload_config
      - lr_monitor
  - /logger:
      - wandb
  - /image_encoder@pretrain_model.image_encoder: resnet
  - /text_encoder@pretrain_model.text_encoder: clinical_bert
  - /data@pretrain_model.train_dataset: mimiccxr


pretrain_model:
    learning_rate: 1e-5
    learning_rate_start: 1e-5
    learning_rate_end: 0
    weight_decay: 1e-6
    max_epochs: 100
    optim: adamw
    warmup_epochs: 0
    image_encoder:
        name: resnet50
        pool_method: attention
    train_dataset:
      image_transform:
        - ${image_transformation.resize}
        - ${image_transformation.random_affine}
        - ${image_transformation.random_horizontal_flip}
      rate: 1.0
      text_transform: []
      max_length: 256
    temperature: 0.01
    local_temperature: 0.01
    batch_size: 32


num_workers: 32
trainer:
  gpus: [0,1,2,3]
  sync_batchnorm: true
  precision: 16
experiment_name: prior_on_mimic_cxr



